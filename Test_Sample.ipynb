{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a Carpeta de Codigos de Estudiantes y al modelo CodeBERTa\n",
    "ruta_carpeta = 'path carpeta de ejemplos estudiantes'\n",
    "ruta_modelo = 'ruta al archivo pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de DataFrame\n",
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo pre-entrenado\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ruta_modelo)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ruta_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrer carpeta de codigos\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.endswith('.py'):\n",
    "        # Leer el archivo\n",
    "        with open(os.path.join(ruta_carpeta, archivo), 'r') as f:\n",
    "            codigo = f.read()\n",
    "\n",
    "        # Preprocesamiento de codigo fuente\n",
    "        for index, row in data.iterrows():\n",
    "            code = row['codigo']\n",
    "\n",
    "        # Limpiar código fuente\n",
    "        codigo = codigo.replace('...', '')  # Eliminar comentarios\n",
    "        codigo = codigo.replace(' ', ' ')  # Eliminar espacios en blanco\n",
    "\n",
    "        # Tokenizar código fuente\n",
    "        tokens = word_tokenize(codigo)\n",
    "\n",
    "        # Eliminar tokens irrelevantes\n",
    "        stop_words = set(stopwords.words('spanish'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de vectorización y almacenamiento\n",
    "def vectorizar_y_almacenar(tokens_list):\n",
    "    # Vectorizar el código con CodeBERTa\n",
    "    representaciones = []\n",
    "\n",
    "    for tokens in tokens_list:\n",
    "        inputs = tokenizer(tokens, return_tensors='pt')  # Codificar el código\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)  # Obtener representaciones del modelo\n",
    "        representaciones.append(outputs.logits.squeeze(0).detach().cpu().numpy())  # Extraer representaciones\n",
    "\n",
    "    # Almacenar datos en DataFrame\n",
    "    data_vectorizada = pd.DataFrame(representaciones, columns=model.config.num_labels_to_classify)\n",
    "    return data_vectorizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorrer carpeta de codigos\n",
    "tokens_list = []\n",
    "\n",
    "for archivo in os.listdir(ruta_carpeta):\n",
    "    if archivo.endswith('.py'):\n",
    "        # Leer el archivo\n",
    "        with open(os.path.join(ruta_carpeta, archivo), 'r') as f:\n",
    "            codigo = f.read()\n",
    "\n",
    "        # Preprocesar el código\n",
    "        tokens_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar y almacenar\n",
    "data_vectorizada = vectorizar_y_almacenar(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir datos con DataFrame principal\n",
    "data = data.join(data_vectorizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de resultados\n",
    "data.groupby('calificacion')['codigo_preprocesado'].apply(list).head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
